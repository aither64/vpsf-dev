{ config, pkgs, lib, ... }:
with lib;
let
  qemuParams = [
    "-drive index=0,id=drive1,file=${config.system.build.squashfs},readonly,media=cdrom,format=raw,if=virtio"
    "-kernel ${config.system.build.kernel}/bzImage -initrd ${config.system.build.initialRamdisk}/initrd"
    ''-append "console=ttyS0 systemConfig=${config.system.build.toplevel} ${toString config.boot.kernelParams} quiet panic=-1"''
    "-nographic"
    "-virtfs local,path=/home/aither/workspace/nixpkgs/vpsadminos,mount_tag=hostNixPath,security_model=passthrough,id=hostNixPath"
    "-virtfs local,path=/home/aither/workspace/vpsadmin/vpsadminos,mount_tag=hostOs,security_model=passthrough,id=hostOs"
    "-virtfs local,path=/home/aither/workspace/vpsadmin/vpsadmin,mount_tag=hostVpsAdmin,security_model=passthrough,id=hostVpsAdmin"
    "-virtfs local,path=/home/aither/workspace/vpsadmin/vpsadminos-templates,mount_tag=hostTemplates,security_model=passthrough,id=hostTemplates"
    "-virtfs local,path=/home/aither/workspace/haveapi/haveapi,mount_tag=hostHaveApi,security_model=passthrough,id=hostHaveApi"
    "-virtfs local,path=/home/aither/workspace/lxc,mount_tag=hostLxc,security_model=passthrough,id=hostLxc"
    "-virtfs local,path=/home/aither/workspace/lxcfs,mount_tag=hostLxcfs,security_model=passthrough,id=hostLxcfs"
    "-virtfs local,path=/home/aither/workspace/htop,mount_tag=hostHtop,security_model=passthrough,id=hostHtop"
    "-device virtio-net,netdev=net1,mac=$(printf '00:60:2f:%02x:%02x:%02x\n' $[RANDOM%256] $[RANDOM%256] $[RANDOM%256])"
    "-netdev bridge,id=net1,br=virbr0"
  ];

  qemuCfg = config.boot.qemu;

  allQemuParams =
    (flatten (imap0 (i: disk: [
      "-drive id=disk${toString i},file=${disk.device},if=none,format=raw"
      "-device ide-drive,drive=disk${toString i},bus=ahci.${toString i}"
    ]) qemuCfg.disks))
    ++
    qemuParams;

  runner = pkgs.writeScript "runner" ''
    #!${pkgs.stdenv.shell}
    ${concatStringsSep "\n" (map (disk:
      ''[ ! -f "${disk.device}" ] && truncate -s${toString disk.size} "${disk.device}"''
    ) (filter (disk: disk.type == "file" && disk.create) qemuCfg.disks))}
    exec ${pkgs.qemu_kvm}/bin/qemu-kvm \
      -name ${config.networking.hostName} \
      -m ${toString qemuCfg.memory} \
      -smp cpus=${toString qemuCfg.cpus},cores=${toString qemuCfg.cpu.cores},threads=${toString qemuCfg.cpu.threads},sockets=${toString qemuCfg.cpu.sockets} \
      -no-reboot \
      -device ahci,id=ahci \
      ${concatStringsSep " \\\n  " allQemuParams}
  '';

  ### new shit

  rubyRunner = pkgs.substituteAll {
    name = "qemu-runner.rb";
    src = ./qemu-runner.rb;
    isExecutable = true;
    ruby = pkgs.ruby;
  };

  rubyRunnerConfig = pkgs.writeText "runner-config.json" (builtins.toJSON {
    qemu = pkgs.qemu_kvm;
    virtiofsd = "${pkgs.virtiofsd}/bin/virtiofsd";
    squashfs = config.system.build.squashfs;
    kernel = "${config.system.build.kernel}/bzImage";
    initrd = "${config.system.build.initialRamdisk}/initrd";
    kernelParams = config.boot.kernelParams ++ [ "quiet" "panic=-1" ];
    toplevel = config.system.build.toplevel;
    disks = qemuCfg.disks;
    name = config.networking.hostName;
    memory = qemuCfg.memory;
    cpus = qemuCfg.cpus;
    cpu = {
      cores = qemuCfg.cpu.cores;
      threads = qemuCfg.cpu.threads;
      sockets = qemuCfg.cpu.sockets;
    };
    virtio-fs = {
      hostNixPath = "/home/aither/workspace/nixpkgs/vpsadminos";
      hostOs = "/home/aither/workspace/vpsadmin/vpsadminos";
      hostVpsAdmin = "/home/aither/workspace/vpsadmin/vpsadmin";
      hostTemplates = "/home/aither/workspace/vpsadmin/vpsadminos-templates";
      hostHaveApi = "/home/aither/workspace/haveapi/haveapi";
      hostLxc = "/home/aither/workspace/lxc";
      hostLxcfs = "/home/aither/workspace/lxcfs";
      hostHtop = "/home/aither/workspace/htop";
    };
  });

  rubyRunnerInvocation = pkgs.writeScript "runner" ''
    #!${pkgs.stdenv.shell}
    exec ${rubyRunner} ${rubyRunnerConfig}
  '';

  ### end of new shit

  nfsCfg = config.services.nfs.server;

in {
  imports = [
    <vpsadmin/nixos/modules/vpsadminos-modules.nix>
    ../qemu.nix
  ];

  users.users.root = {
    openssh.authorizedKeys.keys = [
      "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCyWNChi95oRKXtSGdXtbthvXgWXk4y7uqpKVSIfqPq5GzI/S5WmAGe73Tc6o7aBzby09xpmLI/i41+jzQdSfxrGoCFRvpV+2W221jcdWyF/ojXiUciX2dQGS1gsKVcYNjLmqUrN/fNgY5XjuB10VU3nCenmRGGPep1Sx8CYi61lf5Qxb0AF71ylNJ8/rEXjkXad1vi7zTFteEWj3MmOoK1Fau4ykr6o4v2lSRWEvIxY9S+AFwNVqBtCC210ks1XYInaYuPnz0mdRmoOQIATLdBvIyHuWW5y8M9K+aplkLrUBI8abbrLcGze3lRusx4S3w2V4Pvgt9+DtpRM+kyC5gBhUxO8rY7+pBiIWP0WF87Xs5XfUe+nlhnbp23A/rAppvT6NnpvY10bvWTnKbnBlSyGWPUlYLVdqRwshLNSIKr2YByWorzNtnP63rTe5E8gHnpMs3+4f1Rdz0xgSx8kNZ0vAi7w2moFsjwQzc94Uzy52SkYkGgFYpkystXP05GKyB4N0nStoU25KmdX8dsSYGzF0WERy8KWx0tr1Hv/YONWek7IIHDZin5cTyhkbyktenlAyLJ5uj9Oty4MgKPsE3+GrMdczVTBf5ThhSuvyrZo2CqjTSBc6j7mEyEAAHqM6JNVyPRqhDYmtaK0iLTJCAyqnzQyLD9gxEBuTj/o+3rcw== aither@orion"
    ];
  };

  services.openssh.settings.PermitRootLogin = "yes";

  system.secretsDir = "/secrets";
  vpsadmin.nodectld = {
    enable = true;
    settings = {
      vpsadmin = {
        net_interfaces = [ "eth0" ];

        transaction_public_key = pkgs.writeText "transaction.key" ''
          -----BEGIN PUBLIC KEY-----
          MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA1Iu4qQ2vyoWVyZfIOQUj
          mapvsBN1zPxM3Ewgez0VJ7seB6/lOH3FjJrYA1kKuuzp1qNcPpRu6NU3VxSGCuzB
          qoK7J7Pxzj67sPguIrjA0lm3RJcu4G2qIneqbESBT6+cSG5E5QJpa8BWVpWfxK35
          qg6KXlpL3wF4eBXm2B5aRMJkUAXLq4Hfxcdgkbux+oHayd81BiUOskeVq5vvCGe6
          Ui28VrB4sgDNdMEGQDzIL2V+hjRECRXh1VfFa012z+yHiX1Ys1sbs+9OFHcoDQYJ
          AjChL3bcijCU7BvxmeJhLJe7Q41maFYRrKsfgVgxO78oLMbRAolia8ZAtw8iZXBo
          bQIDAQAB
          -----END PUBLIC KEY-----
        '';
      };

      console = {
        host = "0.0.0.0";
      };
    };
  };

  # vpsadmin.cookie = "ahojky123";
  # vpsadmin.supervisorNode = "vpsadmin@os2.prg.vpsfree.cz";
  # vpsadmin.ssl = {
  #   enable = true;
  #   config =
  #     let
  #       shared = {
  #         certfile = "/var/secrets/${config.networking.hostName}.crt";
  #         keyfile = "/var/secrets/${config.networking.hostName}.key";
  #         cacertfile = "/var/secrets/rootCA.crt";
  #         verify = "verify_peer";
  #       };
  #     in {
  #       server = shared // {
  #         fail_if_no_peer_cert = true;
  #       };
  #       client = shared;
  #     };
  # };

  environment.systemPackages = with pkgs; [
    dhcp
    htop
    tree
    git # for osctl hooks...
    iperf2
  ];

  #system.build.runvm-node = runner;
  system.build.runvm-node = rubyRunnerInvocation;

  tty.autologin.enable = true;
  services.haveged.enable = true;
  services.prometheus.exporters.node = {
    enable = true;
    extraFlags = [ "--collector.textfile.directory=/run/metrics" ];
  };
  boot.kernel.sysctl."sunrpc.nfs_debug" = 1023;

  osctld.settings.cpu_scheduler = {
    enable = true;
    packages."0".cpu_mask = "2-7";
  };
  osctl.exportfs.enable = true;

  networking.lxcbr = true;
  networking.nat = true;
  networking.dhcpd = true;
  networking.static.enable = mkDefault true;
  networking.hosts = {
    "192.168.122.31" = [ "os1" "os1.prg.vpsfree.cz" ];
    "192.168.122.32" = [ "os2" "os2.prg.vpsfree.cz" ];
    "192.168.122.11" = [ "node1-zfs" "node1-zfs.prg.vpsfree.cz" ];
    "192.168.122.12" = [ "node2-zfs" "node2-zfs.prg.vpsfree.cz" ];
  };

  environment.etc = {
    "resolv.conf".text = "nameserver 192.168.122.1";
    "ssh/ssh_host_rsa_key.pub".source = ../ssh_host_rsa_key.pub;
    "ssh/ssh_host_rsa_key" = { mode = "0600"; source = ../ssh_host_rsa_key; };
    "ssh/ssh_host_ed25519_key.pub".source = ../ssh_host_ed25519_key.pub;
    "ssh/ssh_host_ed25519_key" = { mode = "0600"; source = ../ssh_host_ed25519_key; };
    "gitconfig".text = ''
      [safe]
        directory = /mnt/vpsadminos
        directory = /mnt/vpsadminos-templates
        directory = /mnt/vpsadmin
        directory = /mnt/haveapi
    '';
  };

  services.nfs.server = {
    enable = true;
    mountdPort = 20048;
    statdPort = 662;
    lockdPort = 32769;
  };
  networking.firewall.extraCommands = ''
    # rpcbind
    iptables -A nixos-fw -p tcp --dport 111 -j nixos-fw-accept
    iptables -A nixos-fw -p udp --dport 111 -j nixos-fw-accept

    # nfsd
    iptables -A nixos-fw -p tcp --dport ${toString nfsCfg.nfsd.port} -j nixos-fw-accept
    iptables -A nixos-fw -p udp --dport ${toString nfsCfg.nfsd.port} -j nixos-fw-accept

    # mountd
    iptables -A nixos-fw -p tcp --dport ${toString nfsCfg.mountdPort} -j nixos-fw-accept
    iptables -A nixos-fw -p udp --dport ${toString nfsCfg.mountdPort} -j nixos-fw-accept

    # statd
    iptables -A nixos-fw -p tcp --dport ${toString nfsCfg.statdPort} -j nixos-fw-accept
    iptables -A nixos-fw -p udp --dport ${toString nfsCfg.statdPort} -j nixos-fw-accept

    # lockd
    iptables -A nixos-fw -p tcp --dport ${toString nfsCfg.lockdPort} -j nixos-fw-accept
    iptables -A nixos-fw -p udp --dport ${toString nfsCfg.lockdPort} -j nixos-fw-accept
  '';

  boot.qemu = {
    # cpus = 8;
    # cpu.cores = 8;

    cpus = 16;
    cpu.sockets = 2;
    cpu.cores = 8;

    memory = 10240;
  };

  boot.zfs.pools = mkOverride 500 {
    tank = {
      layout = [
        { type = "raidz"; devices = [ "sda" ]; }
      ];
      install = true;
      share = "once";
    };
  };

  # fileSystems."/mnt/nix-path" = {
  #   device = "hostNixPath";
  #   fsType = "9p";
  #   options = [ "trans=virtio" "version=9p2000.L" ];
  # };

  # fileSystems."/mnt/vpsadminos" = {
  #   device = "hostOs";
  #   fsType = "9p";
  #   options = [ "trans=virtio" "version=9p2000.L" ];
  # };

  # fileSystems."/mnt/vpsadmin" = {
  #   device = "hostVpsAdmin";
  #   fsType = "9p";
  #   options = [ "trans=virtio" "version=9p2000.L" ];
  # };

  # fileSystems."/mnt/haveapi" = {
  #   device = "hostHaveApi";
  #   fsType = "9p";
  #   options = [ "trans=virtio" "version=9p2000.L" ];
  # };

  # fileSystems."/mnt/lxc" = {
  #   device = "hostLxc";
  #   fsType = "9p";
  #   options = [ "trans=virtio" "version=9p2000.L" ];
  # };

  # fileSystems."/mnt/lxcfs" = {
  #   device = "hostLxcfs";
  #   fsType = "9p";
  #   options = [ "trans=virtio" "version=9p2000.L" ];
  # };

  # fileSystems."/mnt/htop" = {
  #   device = "hostHtop";
  #   fsType = "9p";
  #   options = [ "trans=virtio" "version=9p2000.L" ];
  # };

  fileSystems."/mnt/nix-path" = {
    device = "hostNixPath";
    fsType = "virtiofs";
  };

  fileSystems."/mnt/vpsadminos" = {
    device = "hostOs";
    fsType = "virtiofs";
  };

  fileSystems."/mnt/vpsadmin" = {
    device = "hostVpsAdmin";
    fsType = "virtiofs";
  };

  fileSystems."/mnt/vpsadminos-templates" = {
    device = "hostTemplates";
    fsType = "virtiofs";
  };

  fileSystems."/mnt/haveapi" = {
    device = "hostHaveApi";
    fsType = "virtiofs";
  };

  fileSystems."/mnt/lxc" = {
    device = "hostLxc";
    fsType = "virtiofs";
  };

  fileSystems."/mnt/lxcfs" = {
    device = "hostLxcfs";
    fsType = "virtiofs";
  };

  fileSystems."/mnt/htop" = {
    device = "hostHtop";
    fsType = "virtiofs";
  };

  boot.postBootCommands = ''
    # modprobe 9pnet_virtio
    mkdir -p /mnt/nix-path /mnt/vpsadminos /mnt/vpsadmin /mnt/vpsadminos-templates /mnt/haveapi /mnt/lxc /mnt/lxcfs /mnt/htop
    mount -a

    mkdir /tmp/core-build /tmp/core-deps
    mount --bind /tmp/core-build /mnt/vpsadmin/core/_build
    mount --bind /tmp/core-deps /mnt/vpsadmin/core/deps

    mkdir /mnt/vpsadmin-bind /mnt/haveapi-bind
    mount --bind /mnt/vpsadmin /mnt/vpsadmin-bind
    mount --bind /mnt/haveapi /mnt/haveapi-bind
  '';

  programs.bash.root.historyPools = mkDefault [ "tank" ];

  os.channel-registration.enable = mkDefault false;

  nix.nixPath = [
    "nixpkgs=/mnt/nix-path"
    "nixpkgs-overlays=/mnt/vpsadminos/os/overlays/common.nix"
  ];
}
